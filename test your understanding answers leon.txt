1 a) Sample space: all possible outcomes from a random experiment. An event is defined as some outcomes from the sample space occuring, all Events form the Event space. The probability Space consists of the Sample space, the event space and the probabilities of each event occuring
1 b) A random variable maps all events from the event space to a number. X:\Omega \rightarrow Val(X)
1 d) Discrete distributions: Bernulli (Head or tail, p(1) = theta), Binomial  (how many heads in n trows, Binom(n_h,n, theta) = \binom{n}{n_h} \theta^{n_H}* (1-\theta)^{n-n_h}), Categorical (which catagory p(k)= \theta_k), Multinomial (how often which distribution in n trials Mu(n|n,theta)= \binom{n}{n_1 ... n_K} \prod^K_{k=1} \theta^{n_k}_k); Continous distributions:  gaussian, Beta, Dirichlet, multivariate gaussian
1 c) Discrete random variables can take infinitely many (uncountable) values where only intervals of values have probability. Discrete random variables only can take finitely many (countable) values, it has a probability mass function where only the discrete output values have probability. 
8 j) K-Means using Lloyd's algorithm yields the same result as a GMM  using isotropic gaussians with only one vaiance term for all classes and hard EM (only one cluster can have nonzero resposibility for a data point). We need the non-isotropic gaussians to accurately model non-sperical clusters. One vairaince term per cluster is needed to accurately model clusters of different densities. The "soft" EM is "only" needed to model overlapping clusters. 
4 o) row:= max number ob lin. indep. rows = col. rank:= max. number of cols that are lin. indep. = rank of matrix. Schein rank. Schein rank could be the number of rank-1 boolean matricies needed to combine into the matrix? https://math.stackexchange.com/questions/4620374/what-is-the-schein-rank-of-the-boolean-matrix-given-below
2 b) classification: predict one class out of a fixed set, regression: predict a continous number, structured
prediction: predict a structured object like a sequence
2 d) The data space expands very fast when we use more dimesions. This leads to problems in having dense data (since there are more feature combinations for which we need to find data), storing data (since there is just so much data) and computing distance measures (why this? just compute time?)
2 h) Generative model p(x,y), model all variables jointly, can draw new data at random; discriminative models model dependent probability p(y|x); Discrimitative function f(x) = y, map inputs to outputs without probabilities  
5 i) The decision boundary is the hyperplane in the feature space where the classifier cannot decide which calss to assign. A classifier is linear if the class label is a linar combination of the features. Log-odds is a linear function of the features in logisitc regression. It is a linear classifier since the the assigned class probability is a function of the log odds.
5 k) For model p(y| x, theta) and dasta D= {(x_i, y_i)_{i=1}}^{N}. -l(theta|D) = -{\sum_{i=1}}^{N} \log p(y_i|x_i, \theta), ERM_{\log} = \frac{1}{N}{\sum_{i=1}}^{N} -\log p(y_i|x_i, \theta)
7 j) A = U * \Sigma * V^\top. A^\top * A = Q_{A^\top * A} * \Lambda * Q_{A^\top * A}, A * A^\top = Q_{A * A^\top} * \Lambda * Q_{A * A^\top}. \Simga = \sqrt(\Lambda), V = Q_{A^\top * A}, U = Q_{A * A^\top}. Mnemonic: ATAV and AATU 
